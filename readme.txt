Sign Language Detection

Project Overview
This repository contains the code and resources for a Sign Language Detection project. The project includes a Jupyter notebook for model training and evaluation, a real-time detection Python script, the saved trained model, and example images with evaluation results.

Repository Structure
1. Sign_Language_Detection.ipynb: Jupyter notebook containing the code for training and evaluating the sign language detection model.
2. real_time_detection.py: Python script for real-time sign language detection using the trained model.
3. bestsign.h5: Saved trained model.
4. dataset_notepad.txt: Text file with the link to the dataset used for training and evaluation.
6. Image_Folder: Directory containing a few example images and evaluation results.

Dataset
The dataset used for this project is provided in the dataset_notepad.txt file. It contains images of various sign language gestures, which are used to train and evaluate the model.

Usage

Training the Model:
Open the Sign_Language_Detection.ipynb notebook.
Follow the instructions to load the dataset, preprocess the images, and train the model.
Save the trained model.

Real-Time Detection:
Ensure the saved model is in the saved_model directory.
Run the real_time_detection.py script to start real-time sign language detection using your webcam.
Example Images and Results:

Refer to the Image_Folder directory to see example images used for evaluation and the corresponding results
